---
title: "mlrng"
output:
  pdf_document:
    toc: true
urlcolor: blue
linkcolor: blue
vignette: >
  %\VignetteIndexEntry{mlrng}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---
```{r, include=FALSE}
library(mlrng)
set.seed(123)
```
# Building Blocks

The package provides R6 classes for the building blocks of machine learning:

* Tasks
* Learners
* Resamplings
* Performance Measures

All objects are stored in dictionaries (a.k.a. hash maps):
`mlr.tasks` comes with some predefined toy tasks, `mlr.learners` with learners, `mlr.resamplings` with different resampling methods and `mlr.measures` with some popular performance measures.

## Tasks

We use the `iris` data set to create a multilabel classification task:
```{r}
iris.task = TaskClassif$new(data = iris, target = "Species")
```

Task objects come with some handy self-explanatory getters:
```{r}
# id
iris.task$id

# dimension
c(iris.task$nrow, iris.task$ncol)

# name of the target column
iris.task$target

# name of the feature columns
iris.task$features

# formula describing the task
iris.task$formula

# column types
iris.task$col.types

# number of classes
iris.task$nclasses

# class levels
iris.task$classes

# positive class (not feasible for multilabel classification)
iris.task$positive

# missing values per column
iris.task$missing.values

# peek into the data
iris.task$head()

# complete data
iris.task$data
```
In `mlrng`, tasks can rely on different data backends to hold tabular data.
Per default, data is stored in SQLite data bases where the data does not occupy any memory unless fetched from the data base.
This allows you to work with hundreds of tasks simultaneously or to conveniently learn on subsets of "big data".
If not configured differently, the temp directory of R is used to store the data base.
Alternatively, you can opt to hold the data in memory (for a small performance boost) or to connect to a real DBMS like PostgresSQL or MariaDB.
This is covered in the help (FIXME).


Here, we first load a predefined task which comes with `mlrng`.
Like most objects in `mlrng`, tasks are stored in a `Dictionary`:
```{r}
print(mlr.tasks)
print(mlr.tasks$ids)
print(mlr.tasks$summary())
```
We use the `$get()` function to retrieve a specific task, here a multiclass classification task based on the dataset `iris`:
```{r}
iris.task = mlr.tasks$get("iris")
print(iris.task)
```
The data itself is stored in an abstract class `Backend`.
It is designed to transparently access in-memory data structures (`data.table` is used internally) as well as real DBMS via the `dplyr` package.
Because most data bases can only be accessed read-only, the operations on these data base backend are limited.
However, you can subset the data to a "view" by excluding rows and columns while the underlying data base must not be altered:
```{r}
# subset to 120 rows and remove column "Petal.Length"
keep = setdiff(c(iris.task$target, iris.task$features), "Petal.Length")
iris.task$subset(rows = 1:120, cols = keep)

iris.task$nrow
iris.task$features
```
Note that you may also alter the data.
In the case of a data base backend, the data is then first fetched from the data base and then stored in a `data.table`.
This affects only the respective rows you operate on.
Thus, if you subsample your data first to only use 0.1% of all observations, you can keep the memory footprint reasonable.


## Learners

All learners are stored in a register called `mlr.learners` and can easily be listed:
```{r}
mlr.learners
mlr.learners$summary()
```

### Dummy classification learner

You can retrieve learners from the dictionary `Learners`:
```{r}
lrn.dummy = mlr.learners$get("classif.dummy")
```

The parameter set is stored in the slot `par.set` and parameters deviating from the default are stored in `par.vals`:
```{r}
lrn.dummy$par.set
lrn.dummy$par.vals
```

Now, we set the parameter `method` to `"sample"`, change the `id` and add the learner to the register:
```{r}
lrn.dummy$par.vals = list(method = "sample")
lrn.dummy$id = "classif.dummy.sample"
mlr.learners$add(lrn.dummy)
mlr.learners$summary()
```
From now on, we can just pass the id `"classif.dummy.sample"` to other functions to use this learner.

### CART

```{r}
lrn.rpart = mlr.learners$get("classif.rpart")
```

## Measures

```{r}
mlr.measures$summary()
measure = mlr.measures$get("mmce")
```

## Resampling

```{r}
mlr.resamplings
mlr.resamplings$summary()
r = mlr.resamplings$get("cv")
print(r)

# change to 3-fold cv
r$iters = 3
```

# Train and Predict

Here, we fit a simple CART on a random subset of the iris task.
The returned object is a `TrainResult`:
```{r}
task = mlr.tasks$get("iris")
lrn = mlr.learners$get("classif.rpart")
set.seed(123); train = sample(task$nrow, 120)
tr = train(task, lrn, subset = train)
print(tr)
tr$train.log
```
We can access the returned `rpart` model via the slot `$rmodel`:
```{r}
print(tr$rmodel)
```

Next, we can use the `TrainResult` to predict on the left-out observations:
```{r}
test = setdiff(1:task$nrow, train)
pr = predict(tr, subset = test)
```



# Resampling

```{r}
rr = resample(task = iris.task, learner = lrn.dummy, resampling = r, measures = list(measure))
rr$data
rr$aggr
```

# Benchmarking

```{r}
tasks = lapply(c("iris", "sonar"), mlr.tasks$get)
learners = lapply(c("classif.dummy", "classif.rpart"), mlr.learners$get)
resamplings = lapply("cv", mlr.resamplings$get)
measures = lapply("mmce", mlr.measures$get)

withr::with_options(list(mlrng.verbose = FALSE), {
  bmr = benchmark(
    tasks = tasks,
    learners = learners,
    resamplings = resamplings,
    measures = measures)
})
bmr$data
```
